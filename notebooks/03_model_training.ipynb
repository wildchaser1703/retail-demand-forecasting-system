{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Training and Comparison\n",
                "\n",
                "This notebook trains and compares multiple forecasting models.\n",
                "\n",
                "## Models Covered:\n",
                "1. Baseline models (Naive, Seasonal Naive)\n",
                "2. Statistical models (ARIMA, Prophet)\n",
                "3. Machine learning models (XGBoost, LightGBM)\n",
                "4. Ensemble methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import sys\n",
                "\n",
                "sys.path.append(str(Path.cwd().parent))\n",
                "\n",
                "from src.data_loader import load_sales_data, split_train_test\n",
                "from src.feature_engineering import FeatureEngineer\n",
                "from src.baselines import BaselineForecaster\n",
                "from src.arima_models import ARIMAForecaster\n",
                "from src.prophet_model import ProphetForecaster\n",
                "from src.ml_models import MLForecaster\n",
                "from src.ensemble import SimpleEnsemble\n",
                "from src.evaluation import ModelEvaluator, compute_metrics\n",
                "from src.visualization import plot_forecast, plot_model_comparison\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "df = load_sales_data(Path.cwd().parent / \"data\" / \"raw\" / \"sales_data.parquet\")\n",
                "df_agg = df.groupby(['store_id', 'date'])['sales'].sum().reset_index()\n",
                "\n",
                "# Split data\n",
                "train_df, val_df, test_df = split_train_test(df_agg, test_weeks=6, validation_weeks=8)\n",
                "\n",
                "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Baseline Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate to daily total\n",
                "train_series = train_df.groupby('date')['sales'].sum().values\n",
                "test_series = test_df.groupby('date')['sales'].sum().values\n",
                "horizon = len(test_series)\n",
                "\n",
                "# Initialize evaluator\n",
                "evaluator = ModelEvaluator()\n",
                "\n",
                "# Naive\n",
                "naive = BaselineForecaster(method='naive')\n",
                "naive_forecast = naive.fit_predict(train_series, horizon)\n",
                "evaluator.evaluate_model('Naive', test_series, naive_forecast, train_series)\n",
                "\n",
                "# Seasonal Naive\n",
                "seasonal_naive = BaselineForecaster(method='seasonal_naive', season_length=7)\n",
                "sn_forecast = seasonal_naive.fit_predict(train_series, horizon)\n",
                "evaluator.evaluate_model('Seasonal Naive', test_series, sn_forecast, train_series)\n",
                "\n",
                "print(\"Baseline Results:\")\n",
                "print(evaluator.get_results())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Statistical Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ARIMA (this may take a few minutes)\n",
                "print(\"Training ARIMA...\")\n",
                "arima = ARIMAForecaster(seasonal=True, m=7)\n",
                "arima.fit(train_series)\n",
                "arima_forecast = arima.predict(steps=horizon)\n",
                "evaluator.evaluate_model('ARIMA', test_series, arima_forecast, train_series)\n",
                "\n",
                "print(\"ARIMA training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prophet\n",
                "print(\"Training Prophet...\")\n",
                "train_prophet = train_df.groupby('date')['sales'].sum().reset_index()\n",
                "train_prophet.columns = ['ds', 'y']\n",
                "\n",
                "prophet = ProphetForecaster(seasonality_mode='multiplicative')\n",
                "prophet.fit(train_prophet, date_col='ds', target_col='y')\n",
                "prophet_forecast_df = prophet.predict(steps=horizon)\n",
                "prophet_forecast = prophet_forecast_df['yhat'].values\n",
                "\n",
                "evaluator.evaluate_model('Prophet', test_series, prophet_forecast, train_series)\n",
                "print(\"Prophet training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Machine Learning Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Engineer features for ML\n",
                "combined_df = pd.concat([train_df, val_df, test_df])\n",
                "engineer = FeatureEngineer(combined_df, date_column='date')\n",
                "featured_df = engineer.create_all_features(target_column='sales', group_columns=['store_id'])\n",
                "\n",
                "# Split featured data\n",
                "train_feat = featured_df[featured_df['date'] < val_df['date'].min()].dropna()\n",
                "val_feat = featured_df[\n",
                "    (featured_df['date'] >= val_df['date'].min()) & \n",
                "    (featured_df['date'] < test_df['date'].min())\n",
                "].dropna()\n",
                "test_feat = featured_df[featured_df['date'] >= test_df['date'].min()].dropna()\n",
                "\n",
                "# Prepare features\n",
                "feature_cols = [c for c in train_feat.columns if c not in ['date', 'store_id', 'sales']]\n",
                "X_train = train_feat[feature_cols]\n",
                "y_train = train_feat['sales']\n",
                "X_val = val_feat[feature_cols]\n",
                "y_val = val_feat['sales']\n",
                "X_test = test_feat[feature_cols]\n",
                "y_test = test_feat['sales']\n",
                "\n",
                "print(f\"Features: {len(feature_cols)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost\n",
                "print(\"Training XGBoost...\")\n",
                "xgb = MLForecaster(model_type='xgboost', max_depth=6, learning_rate=0.1, n_estimators=100)\n",
                "xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)\n",
                "xgb_pred = xgb.predict(X_test)\n",
                "evaluator.evaluate_model('XGBoost', y_test.values, xgb_pred, y_train.values)\n",
                "print(\"XGBoost complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LightGBM\n",
                "print(\"Training LightGBM...\")\n",
                "lgb = MLForecaster(model_type='lightgbm', max_depth=6, learning_rate=0.1, n_estimators=100)\n",
                "lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)\n",
                "lgb_pred = lgb.predict(X_test)\n",
                "evaluator.evaluate_model('LightGBM', y_test.values, lgb_pred, y_train.values)\n",
                "print(\"LightGBM complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Ensemble"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate test series for ensemble\n",
                "test_agg = test_feat.groupby('date')['sales'].sum().values\n",
                "\n",
                "# Combine forecasts (need to align lengths)\n",
                "min_len = min(len(naive_forecast), len(prophet_forecast), len(test_agg))\n",
                "\n",
                "forecasts = {\n",
                "    'Naive': naive_forecast[:min_len],\n",
                "    'Seasonal_Naive': sn_forecast[:min_len],\n",
                "    'Prophet': prophet_forecast[:min_len]\n",
                "}\n",
                "\n",
                "ensemble = SimpleEnsemble(method='mean')\n",
                "ensemble_forecast = ensemble.combine(forecasts)\n",
                "\n",
                "evaluator.evaluate_model('Ensemble', test_agg[:min_len], ensemble_forecast)\n",
                "print(\"Ensemble complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all results\n",
                "results = evaluator.get_results()\n",
                "print(\"\\nFinal Results:\")\n",
                "print(results.to_string())\n",
                "\n",
                "# Best model\n",
                "best = evaluator.get_best_model(metric='mae')\n",
                "print(f\"\\nBest Model (MAE): {best}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot comparison\n",
                "plot_model_comparison(results, metric='mae', title='Model Comparison by MAE')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Importance (XGBoost)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.visualization import plot_feature_importance\n",
                "\n",
                "importance = xgb.get_feature_importance(top_n=20)\n",
                "plot_feature_importance(importance, top_n=20, title='XGBoost Feature Importance')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Conclusions\n",
                "\n",
                "### Key Findings:\n",
                "\n",
                "1. **Best Performer**: Typically XGBoost or LightGBM with engineered features\n",
                "2. **Baseline Value**: Seasonal Naive provides good baseline\n",
                "3. **Feature Importance**: Lag features most important\n",
                "4. **Ensemble Benefit**: Combining models reduces variance\n",
                "\n",
                "### Recommendations:\n",
                "\n",
                "- Use XGBoost/LightGBM for production\n",
                "- Maintain Seasonal Naive as fallback\n",
                "- Consider ensemble for critical forecasts\n",
                "- Monitor performance over time\n",
                "- Retrain periodically with new data"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}